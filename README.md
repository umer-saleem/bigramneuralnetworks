# Neural Bigram Language Model (From Scratch with PyTorch)

This project implements a **character-level neural bigram language model** from scratch using **PyTorch**, without relying on high-level abstractions like `nn.Module` or optimizers. The model learns character-to-character transition probabilities from a dataset of names and generates new names by sampling from the learned distribution.

The goal of this project is **educational**: to deeply understand how bigram language models, cross-entropy loss, and maximum likelihood training work under the hood.

## Project Overview

A **bigram language model** estimates the probability of the next character given the current character:

\[
P(c_{t+1} \mid c_t)
\]

In this implementation:
- Each character is represented as a **one-hot vector**
- A **single linear layer** maps the current character to logits over the next character
- **Cross-entropy loss** is used to train the model via **maximum likelihood estimation (MLE)**
- New names are generated by **sampling from the learned probability distribution**

## Model Architecture

The model is a **single-layer neural bigram model**:

**One-hot (current character) → Linear layer (W, b) → Logits over vocabulary → Softmax → Probabilities**

Mathematically, the neural bigram model is defined as:

$$
\text{logits} = W[c_t] + b
$$

$$
P(c_{t+1} \mid c_t) = \text{softmax}(\text{logits})
$$

Where:
- W has shape (vocab_size, vocab_size)
- Each row of W represents the logits for the next character given the current character


 


