# Neural Bigram Language Model (From Scratch with PyTorch)

This project implements a **character-level neural bigram language model** from scratch using **PyTorch**, without relying on high-level abstractions like `nn.Module` or optimizers. The model learns character-to-character transition probabilities from a dataset of names and generates new names by sampling from the learned distribution.

The goal of this project is **educational**: to deeply understand how bigram language models, cross-entropy loss, and maximum likelihood training work under the hood.

## Project Overview

A **bigram language model** estimates the probability of the next character given the current character:

$$
\text P(c_{t+1} \mid c_t)
$$

In this implementation:
- Each character is represented as a **one-hot vector**
- A **single linear layer** maps the current character to logits over the next character
- **Cross-entropy loss** is used to train the model via **maximum likelihood estimation (MLE)**
- New names are generated by **sampling from the learned probability distribution**

## Model Architecture

The model is a **single-layer neural bigram model**:

**One-hot (current character) → Linear layer (W, b) → Logits over vocabulary → Softmax → Probabilities**

Mathematically, the neural bigram model is defined as:

$$
\text{logits} = W[c_t] + b
$$

$$
P(c_{t+1} \mid c_t) = \text{softmax}(\text{logits})
$$

Where:  
- W has shape (vocab_size, vocab_size)  
- Each row of W represents the logits for the next character given the current character

## Dataset

- **Input file:** ```names.txt```
- Each line contains a single name
- The dataset is split into training and test sets using a configurable ratio (default: 90% train, 10% test)

Special tokens:
- ```<S>``` — start of word
- ```<E>``` — end of word

Each word is converted into bigram pairs such as:

```
<S> → a
a → l
l → e
e → <E>
```

## Data Processing Pipeline

**1.** Read all words from ```names.txt```  
**2.** Build a character vocabulary  
**3.** Add ```<S>``` and ```<E>``` tokens    
**4.** Create mappings:    
 - ```stoi```: character → integer  
 - ```itos```: integer → character  

**5.** Generate (x, y) bigram pairs  
**6.** Split data into training and test sets  
**7.** Convert inputs to one-hot encodings  
 
## Training Details

- **Loss function:** Cross-Entropy Loss  
- **Optimization:** Manual stochastic gradient descent (SGD)  
- **Learning rate:** ```0.1```  
- **Epochs:** ```500```  
- **Framework:** PyTorch (low-level tensor operations)

Cross-entropy loss corresponds to **negative log-likelihood**, meaning training maximizes the probability of the observed bigrams.

## Evaluation  
After training:  
- The model is evaluated on the test set using **cross-entropy loss**  
- Accuracy is intentionally omitted, as it is not a meaningful metric for language models  
- Lower test loss indicates better probabilistic modeling of character transitions

## Name Generation

New names are generated by:  
- Starting with the ```<S>``` token  
- Predicting the next character distribution  
- Sampling from the distribution using ```torch.multinomial```  
- Repeating until <E> is generated or a maximum length is reached  

This produces **random but realistic-looking names** based on learned character patterns.

## How to Run

**1.** Ensure you have PyTorch installed:  
```pip install torch```  
**2.** Place names.txt in the same directory   
**3.** Run the script:  
```python bigram_nn.py```

## Key Learning Outcomes

This project demonstrates:
- How language models work at the character level  
- How **cross-entropy** loss implements maximum likelihood estimation  
- Why logits (not probabilities) are model parameters  
- How softmax converts logits into probability distributions  
- How to sample text from a trained language model  
- Why one-hot encoding acts as a lookup table in bigram models

## Limitations

- The model only considers one character of context  
- It does not generalize across characters  
- No embeddings or hidden layers are used

These limitations are intentional for clarity and learning purposes.
