# Neural Bigram Language Model (From Scratch with PyTorch)

This project implements a **character-level neural bigram language model** from scratch using **PyTorch**, without relying on high-level abstractions like `nn.Module` or optimizers. The model learns character-to-character transition probabilities from a dataset of names and generates new names by sampling from the learned distribution.

The goal of this project is **educational**: to deeply understand how bigram language models, cross-entropy loss, and maximum likelihood training work under the hood.

## Project Overview

A **bigram language model** estimates the probability of the next character given the current character:

\[
P(c_{t+1} \mid c_t)
\]

In this implementation:
- Each character is represented as a **one-hot vector**
- A **single linear layer** maps the current character to logits over the next character
- **Cross-entropy loss** is used to train the model via **maximum likelihood estimation (MLE)**
- New names are generated by **sampling from the learned probability distribution**

## Model Architecture

The model is a **single-layer neural bigram model**:

